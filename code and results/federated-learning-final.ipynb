{"metadata":{"colab":{"name":"Federated Learning(SVM) - CovidVsPneumonia_version1","provenance":[],"collapsed_sections":["LIq8hS84zfWw","lUjjTsJ4vSL1","Wlheg1o7va2A"]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3324348,"sourceType":"datasetVersion","datasetId":576013}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport random\nimport copy\nimport sys\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.utils import shuffle\nimport cv2\nimport os\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nimport time","metadata":{"id":"18Og6qjRpilC","execution":{"iopub.status.busy":"2023-11-29T05:15:39.847630Z","iopub.execute_input":"2023-11-29T05:15:39.848518Z","iopub.status.idle":"2023-11-29T05:15:56.636373Z","shell.execute_reply.started":"2023-11-29T05:15:39.848481Z","shell.execute_reply":"2023-11-29T05:15:56.634705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create clients from two classes of data","metadata":{"id":"LIq8hS84zfWw"}},{"cell_type":"code","source":"def get_clients(class1, class2, n_clients = 3):\n\n  clients_X = []\n  clients_y = []\n\n  clientsXtest = []\n  clientsYtest = []\n\n  clusters_1 = KMeans(n_clusters=n_clients, random_state=0).fit_predict(class1)\n  clusters_2 = KMeans(n_clusters=n_clients, random_state=0).fit_predict(class2)\n\n  for i in range(n_clients):\n\n    X_train0, X_test0, y_train0, y_test0 = train_test_split(class1[clusters_1 == i],np.zeros((class1[clusters_1 == i].shape[0],)),test_size=0.2)\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(class2[clusters_2 == i],np.ones((class2[clusters_2 == i].shape[0],)),test_size=0.2)\n\n    clients_X.append([X_train0, X_train1])\n    clients_y.append([y_train0, y_train1])\n\n    clientsXtest.extend([X_test0,X_test1])\n    clientsYtest.extend([y_test0,y_test1])\n\n  X_test = np.concatenate(clientsXtest,axis=0)\n  y_test = np.concatenate(clientsYtest,axis=0)\n\n  return clients_X,clients_y,X_test,y_test","metadata":{"id":"fJsvk7I2np-p","execution":{"iopub.status.busy":"2023-11-29T05:15:56.638630Z","iopub.execute_input":"2023-11-29T05:15:56.639301Z","iopub.status.idle":"2023-11-29T05:15:56.650458Z","shell.execute_reply.started":"2023-11-29T05:15:56.639265Z","shell.execute_reply":"2023-11-29T05:15:56.649117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_total_from_clients(clients_X,clients_y):\n  x_train0 = [i[0] for i in clients_X]\n  x_train0 = np.concatenate(x_train0, axis=0)\n  x_train1 = [i[1] for i in clients_X]\n  x_train1 = np.concatenate(x_train1, axis=0)\n  y_train0 = [i[0] for i in clients_y]\n  y_train0 = np.concatenate(y_train0, axis=0)\n  y_train1 = [i[1] for i in clients_y]\n  y_train1 = np.concatenate(y_train1, axis=0)\n\n  return ([x_train0,x_train1],[y_train0,y_train1])","metadata":{"id":"nngBbU3-3_sj","execution":{"iopub.status.busy":"2023-11-29T05:15:56.652273Z","iopub.execute_input":"2023-11-29T05:15:56.652765Z","iopub.status.idle":"2023-11-29T05:15:56.671124Z","shell.execute_reply.started":"2023-11-29T05:15:56.652724Z","shell.execute_reply":"2023-11-29T05:15:56.669812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# svm","metadata":{"id":"lUjjTsJ4vSL1"}},{"cell_type":"code","source":"class SVM:\n\n  def __init__(self, X_train, y_train, X_test, y_test, val=True, val_type='k_fold', val_distribution='balanced', k=5, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n\n    self.lr = learning_rate\n    self.lambda_param = lambda_param\n    self.n_iters = n_iters\n\n    self.X_train = X_train\n    self.y_train = y_train\n\n    self.X_test = X_test\n    self.y_test = y_test\n\n    self.val_distribution = val_distribution\n    self.val = val\n    self.val_type=val_type\n    self.val_distribution=val_distribution\n    self.k=k\n\n    self.w = np.array([])\n    self.b = None\n\n\n  def Gradient_update(self, X_train, y_train, X_val=None, y_val=None):\n\n    n_samples, n_features = X_train.shape\n    y_ = np.where(y_train <= 0, -1, 1)\n\n    if self.w.size == 0 and self.b is None :\n      self.w = np.zeros(n_features)\n      self.b = 0\n\n    w_best = np.zeros(n_features)\n    b_best = 0\n\n    acc_list = []\n    for i in range(0,self.n_iters):\n      for idx, x_i in enumerate(X_train):\n        condition = y_[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n        if condition:\n          self.w -= self.lr * (2 * self.lambda_param * self.w)\n        else:\n          self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(x_i, y_[idx]))\n          self.b -= self.lr * y_[idx]\n\n      if i%10 == 0 and self.val:\n        approx_w = np.dot(X_val, self.w) - self.b\n        approx_w = np.sign(approx_w)\n        res_w = np.where(approx_w<0, 0, approx_w)\n\n        approx_w_best = np.dot(X_val, w_best) - b_best\n        approx_w_best = np.sign(approx_w_best)\n        res_w_best = np.where(approx_w_best<0, 0, approx_w_best)\n\n        if (accuracy_score(y_val, res_w_best) < accuracy_score(y_val, res_w)):\n          w_best = copy.deepcopy(self.w)\n          b_best = copy.deepcopy(self.b)\n        else:\n          self.w = copy.deepcopy(w_best)\n          self.b = copy.deepcopy(b_best)\n          break\n\n  def Cross_validation(self, val_split):\n\n    if (self.val_distribution == 'balanced'):\n      X_train0, X_val0, y_train0, y_val0 = train_test_split(self.X_train[0], self.y_train[0], test_size=val_split)\n      X_train1, X_val1, y_train1, y_val1 = train_test_split(self.X_train[1], self.y_train[1], test_size=val_split)\n\n      X_train = np.concatenate((X_train0,X_train1),axis=0)\n      y_train = np.concatenate((y_train0,y_train1),axis=0)\n\n      X_val = np.concatenate((X_val0,X_val1),axis=0)\n      y_val = np.concatenate((y_val0,y_val1),axis=0)\n\n    elif (self.val_distribution == 'unbalanced'):\n      X_train = np.concatenate((self.X_train[0],self.X_train[1]),axis=0)\n      y_train = np.concatenate((self.y_train[0],self.y_train[1]),axis=0)\n\n      X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split)\n\n    X_train, y_train = self.random_shuffle(X_train, y_train)\n    self.Gradient_update(X_train, y_train, X_val, y_val)\n\n  def k_fold_cross_validation(self):\n\n    if (self.val_distribution == 'unbalanced'):\n      X_train = np.concatenate((self.X_train[0],self.X_train[1]),axis=0)\n      y_train = np.concatenate((self.y_train[0],self.y_train[1]),axis=0)\n\n      X_train_, X_train0, y_train_, y_train0 = train_test_split(X_train, y_train, test_size=round(1/self.k,2), shuffle=True)\n\n      X_train = []\n      y_train = []\n\n      X_train.append(copy.deepcopy(X_train0))\n      y_train.append(copy.deepcopy(y_train0))\n      k = self.k - 1\n\n      X_train0 = np.array_split(X_train_,k)\n      y_train0 = np.array_split(y_train_,k)\n\n      for i in range(k):\n        X_train.append(X_train0[i])\n        y_train.append(y_train0[i])\n\n    elif (self.val_distribution == 'balanced'):\n      X_train0 = np.array_split(self.X_train[0],self.k)\n      X_train1 = np.array_split(self.X_train[1],self.k)\n      y_train0 = np.array_split(self.y_train[0],self.k)\n      y_train1 = np.array_split(self.y_train[1],self.k)\n      X_train = []\n      y_train = []\n      for i in range(self.k):\n        X_train.append(np.concatenate((X_train0[i],X_train1[i]),axis=0))\n        y_train.append(np.concatenate((y_train0[i],y_train1[i]),axis=0))\n\n    if self.w.size == 0 and self.b == None:\n      w = np.zeros(self.X_train[0].shape[1])\n      b = 0\n    else:\n      w = copy.deepcopy(self.w)\n      b = self.b\n\n    w_list = []\n    b_list = []\n    acc_list = []\n    for i in range(self.k):\n      X_train_temp = np.zeros((1,X_train[0].shape[1]))\n      y_train_temp = np.array([])\n\n      for j in range(self.k):\n        if (j!=i):\n          X_train_temp = np.concatenate((X_train_temp,X_train[j]),axis=0)\n          y_train_temp = np.concatenate((y_train_temp,y_train[j]),axis=0)\n        else:\n          X_val = X_train[j]\n          y_val = y_train[j]\n\n\n      X_train_temp = np.delete(X_train_temp,0,0)\n      X_train_temp, y_train_temp = self.random_shuffle(X_train_temp, y_train_temp)\n      self.Gradient_update(X_train_temp, y_train_temp, X_val, y_val)\n      print(self.accuracy())\n      w_list.append(self.w)\n      b_list.append(self.b)\n\n      test_w = np.dot(X_val, self.w) - self.b\n      test_w = np.sign(test_w)\n      res_val = np.where(test_w<0,0,test_w)\n\n      acc_list.append(accuracy_score(y_val, res_val))\n\n      self.w = copy.deepcopy(w)\n      self.b = b\n\n    self.w = copy.deepcopy(w_list[acc_list.index(max(acc_list))])\n    self.b = b_list[acc_list.index(max(acc_list))]\n\n\n  def fit(self):\n    if self.val_type == 'k_fold' and self.val:\n      self.k_fold_cross_validation()\n\n    elif self.val_type == 'cross_val' and self.val:\n      self.Cross_validation(0.2)\n\n    elif not self.val:\n      X_train = np.concatenate((self.X_train[0],self.X_train[1]),axis=0)\n      y_train = np.concatenate((self.y_train[0],self.y_train[1]),axis=0)\n      X_train, y_train = self.random_shuffle(X_train, y_train)\n      self.Gradient_update(X_train, y_train)\n\n  def random_shuffle(self, X_train, y_train):\n    self.x_tr, self.x_te, self.y_tr, self.y_te = train_test_split(X_train,y_train,test_size=0.5)\n    return np.concatenate((self.x_tr, self.x_te),axis=0), np.concatenate((self.y_tr, self.y_te),axis=0)\n\n  def predict(self):\n     approx = np.dot(self.X_test, self.w) - self.b\n     approx = np.sign(approx)\n     return np.where(approx<0, 0, approx)\n\n  def accuracy(self):\n    return accuracy_score(self.y_test, self.predict())*100\n\n  def precision(self):\n    return precision_score(self.y_test, self.predict())*100","metadata":{"id":"iTq-AWnMChk-","execution":{"iopub.status.busy":"2023-11-29T05:15:56.674382Z","iopub.execute_input":"2023-11-29T05:15:56.674850Z","iopub.status.idle":"2023-11-29T05:15:56.722430Z","shell.execute_reply.started":"2023-11-29T05:15:56.674814Z","shell.execute_reply":"2023-11-29T05:15:56.720870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FL","metadata":{"id":"Wlheg1o7va2A"}},{"cell_type":"code","source":"class Federated_SVM:\n\n  def __init__(self, n_clients=3, val=True, val_type='k_fold', val_distribution='balanced', k=5, learning_rate=0.001, lambda_param=0.01, n_iters=100):\n    self.n_clients = n_clients\n    self.learning_rate = learning_rate\n    self.lambda_param = lambda_param\n    self.n_iters = n_iters\n    self.val = val\n    self.val_type = val_type\n    self.val_distribution = val_distribution\n    self.client_distribution = []\n    self.k = k\n    self.X_test = None\n    self.y_test = None\n    self.noise = None\n\n  def create_clients(self, X_train, y_train, X_test, y_test):\n    self.clients=[]\n    for i in range(self.n_clients):\n      self.client_distribution.append(X_train[i][0].shape[0] + X_train[i][1].shape[0])\n      self.clients.append(SVM(X_train[i],y_train[i], X_test, y_test, self.val, self.val_type, self.val_distribution, self.k, self.learning_rate, self.lambda_param, self.n_iters))\n    self.X_test = copy.deepcopy(X_test)\n    self.y_test = copy.deepcopy(y_test)\n\n  def average_aggregator(self, parameter_list):\n    if not parameter_list:\n        # Handle the case when parameter_list is empty\n        return np.zeros(self.X_test.shape[1]), 0\n\n    w = np.zeros(parameter_list[0].shape[0])\n    b = 0\n    for i in range(0, 2 * self.n_clients, 2):\n        w = np.add(w, parameter_list[i] * self.client_distribution[i // 2] / sum(self.client_distribution))\n        b = b + parameter_list[i + 1]\n\n    return w, b / self.n_clients\n\n  def fit(self, g_iters, aggregator):\n    start_time_fl_svm = time.time()\n    w_best = np.zeros(self.X_test.shape[1])\n    b_best = 0\n\n    # Lists to store accuracy and precision values after each global round\n    self.accuracy_history = []\n    self.precision_history = []\n\n    # Lists to store accuracy values from each client after each epoch\n    self.client_accuracy_history = [[] for _ in range(self.n_clients)]\n\n    w_agg = np.zeros(self.X_test.shape[1])  # Initialize w_agg\n    b_agg = 0  # Initialize b_agg\n\n    for i in range(g_iters):\n        print('global round', i + 1)\n        for j in range(self.n_clients):\n            if i == 0:\n                self.clients[j].fit()\n            else:\n                self.clients[j].w = copy.deepcopy(w_agg)\n                self.clients[j].b = copy.deepcopy(b_agg)\n                self.clients[j].fit()\n            print('client', j + 1, \"acc pr\", self.clients[j].accuracy(), self.clients[j].precision())\n\n            # Collect accuracy from each client after each epoch\n            self.client_accuracy_history[j].append(self.clients[j].accuracy())\n\n        # Collect accuracy and precision after each epoch\n        epoch_accuracies = [self.clients[k].accuracy() for k in range(self.n_clients)]\n        epoch_precisions = [self.clients[k].precision() for k in range(self.n_clients)]\n\n        # Calculate average accuracy and precision for the federated model\n        epoch_accuracy = np.mean(epoch_accuracies)\n        epoch_precision = np.mean(epoch_precisions)\n\n        self.accuracy_history.append(epoch_accuracy)\n        self.precision_history.append(epoch_precision)\n\n        parameter_list = []\n        for k in range(self.n_clients):\n            parameter_list.append(self.clients[k].w)\n            parameter_list.append(self.clients[k].b)\n\n        w_agg, b_agg = aggregator(parameter_list)\n\n        if epoch_accuracy > self.accuracy(w_best, b_best) or i == 0:\n            w_best = copy.deepcopy(w_agg)\n            b_best = copy.deepcopy(b_agg)\n\n        print('global test acc pr', epoch_accuracy, epoch_precision)\n        print('Global parameter size:', sys.getsizeof(w_agg), sys.getsizeof(b_agg))\n        end_time_fl_svm = time.time()\n        time_taken_fl_svm = end_time_fl_svm - start_time_fl_svm\n        print(\"Time taken for federated SVM:\", time_taken_fl_svm, \"seconds\")\n\n    # Plotting accuracy and precision line graphs\n    self.plot_metrics_accuracy_graph(self.accuracy_history)\n    self.plot_metrics_precision_graph(self.precision_history)\n\n    # Plotting accuracy from each client after each epoch\n    self.plot_client_accuracy_line_graph()\n\n  def plot_client_accuracy_line_graph(self):\n    x_values = range(1, len(self.client_accuracy_history[0]) + 1)\n\n    plt.figure(figsize=(10, 6))\n    for i in range(self.n_clients):\n        plt.plot(x_values, self.client_accuracy_history[i], label=f'Client {i + 1}', marker='o')\n\n    plt.xlabel('Local Rounds')\n    plt.ylabel('Accuracy')\n    plt.title(' Local Client Accuracy vs Rounds ')\n    plt.legend()\n    plt.show()\n  \n  def plot_metrics_accuracy_graph(self, accuracy_values):\n        x_values = range(1, len(accuracy_values) + 1)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(x_values, accuracy_values, label='Accuracy', marker='o')\n\n        plt.xlabel('Global Rounds')\n        plt.ylabel('Accuracy(%)')\n        plt.title('Federated SVM Accuracy Over Global Rounds')\n        plt.legend()\n        plt.show()\n        \n  def plot_metrics_precision_graph(self, precision_values):\n        x_values = range(1, len(precision_values) + 1)\n\n        plt.figure(figsize=(10, 6))\n        plt.plot(x_values, precision_values, label='Precision', marker='o')\n\n        plt.xlabel('Global Rounds')\n        plt.ylabel('Precision(%)')\n        plt.title('Federated SVM precision Over Global Rounds')\n        plt.legend()\n        plt.show()\n\n  def predict(self,w,b):\n     approx = np.dot(self.X_test, w) - b\n     approx = np.sign(approx)\n     return np.where(approx<0, 0, 1)\n\n  def accuracy(self,w,b):\n    return accuracy_score(self.y_test, self.predict(w,b))*100\n\n  def precision(self,w,b):\n    return precision_score(self.y_test, self.predict(w,b))*100\n","metadata":{"id":"eiCxNHdVB0mO","execution":{"iopub.status.busy":"2023-11-29T05:15:56.723982Z","iopub.execute_input":"2023-11-29T05:15:56.724339Z","iopub.status.idle":"2023-11-29T05:15:56.764701Z","shell.execute_reply.started":"2023-11-29T05:15:56.724310Z","shell.execute_reply":"2023-11-29T05:15:56.763217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# load covid data","metadata":{"id":"4cVhZT2WzuGR"}},{"cell_type":"code","source":"ima = plt.imread(\"/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia/images/Viral Pneumonia-352.png\")\nplt.imshow(ima,cmap='gray')\nprint(ima.shape)\n","metadata":{"id":"Pbs0-Eu4zssH","outputId":"ad2eb302-7458-467c-805b-020c9b7c0529","execution":{"iopub.status.busy":"2023-11-29T05:15:56.766605Z","iopub.execute_input":"2023-11-29T05:15:56.767436Z","iopub.status.idle":"2023-11-29T05:15:57.189789Z","shell.execute_reply.started":"2023-11-29T05:15:56.767392Z","shell.execute_reply":"2023-11-29T05:15:57.188737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ima = plt.imread(\"/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images/COVID-1.png\")\nplt.imshow(ima,cmap='gray')\nprint(ima.shape)","metadata":{"id":"mi7OXe_Iz9gF","outputId":"3e7cf31d-a75b-4765-e6c9-54c122ad1003","execution":{"iopub.status.busy":"2023-11-29T05:15:57.191306Z","iopub.execute_input":"2023-11-29T05:15:57.192139Z","iopub.status.idle":"2023-11-29T05:15:57.555433Z","shell.execute_reply.started":"2023-11-29T05:15:57.192096Z","shell.execute_reply":"2023-11-29T05:15:57.554233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# resize covid data","metadata":{"id":"z1kvB7VH0jrW"}},{"cell_type":"code","source":"images_path = os.listdir('/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images/')\ncovid = []\n\nfor img_path in images_path:\n    # Read and process each image\n    image = plt.imread(\"/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/COVID/images/\" + img_path)\n    covid.append(cv2.resize(image, (100, 100)).flatten())\n\ncovid = np.stack(covid, axis=0)","metadata":{"id":"MNpNhDC20mcA","execution":{"iopub.status.busy":"2023-11-29T05:15:57.557299Z","iopub.execute_input":"2023-11-29T05:15:57.558095Z","iopub.status.idle":"2023-11-29T05:16:36.436080Z","shell.execute_reply.started":"2023-11-29T05:15:57.558057Z","shell.execute_reply":"2023-11-29T05:16:36.434724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_path = os.listdir('/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia/images/')\npnemo = []\n\nfor img_path in images_path:\n    # Read each image\n    image = plt.imread(\"/kaggle/input/covid19-radiography-database/COVID-19_Radiography_Dataset/Viral Pneumonia/images/\" + img_path)\n    \n    # Check if the flattened image shape is 89401 before resizing and adding to the list\n    if image.flatten().shape[0] == 89401:\n        pnemo.append(cv2.resize(image, (100, 100)).flatten())\n\npnemo = np.stack(pnemo, axis=0)","metadata":{"id":"hNVO_XcB00rA","execution":{"iopub.status.busy":"2023-11-29T05:16:36.438027Z","iopub.execute_input":"2023-11-29T05:16:36.438514Z","iopub.status.idle":"2023-11-29T05:16:50.088347Z","shell.execute_reply.started":"2023-11-29T05:16:36.438469Z","shell.execute_reply":"2023-11-29T05:16:50.087145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(covid.shape)\nprint(pnemo.shape)","metadata":{"id":"MCqpqRD42e5_","outputId":"c8b63210-cce4-4e43-e256-ada3bc27b65d","execution":{"iopub.status.busy":"2023-11-29T05:16:50.092303Z","iopub.execute_input":"2023-11-29T05:16:50.092704Z","iopub.status.idle":"2023-11-29T05:16:50.098567Z","shell.execute_reply.started":"2023-11-29T05:16:50.092671Z","shell.execute_reply":"2023-11-29T05:16:50.097281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# clients of data","metadata":{"id":"oYx030HZ2hnz"}},{"cell_type":"code","source":"clients_X,clients_y,X_test,y_test = get_clients(covid, pnemo, n_clients = 3)","metadata":{"id":"5u7t1psx2k8R","execution":{"iopub.status.busy":"2023-11-29T05:16:50.100053Z","iopub.execute_input":"2023-11-29T05:16:50.100433Z","iopub.status.idle":"2023-11-29T05:17:13.746886Z","shell.execute_reply.started":"2023-11-29T05:16:50.100400Z","shell.execute_reply":"2023-11-29T05:17:13.745525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain_gl, ytrain_gl = get_total_from_clients(clients_X,clients_y)","metadata":{"id":"lik6NwII5pxe","execution":{"iopub.status.busy":"2023-11-29T05:17:13.748625Z","iopub.execute_input":"2023-11-29T05:17:13.749117Z","iopub.status.idle":"2023-11-29T05:17:13.859221Z","shell.execute_reply.started":"2023-11-29T05:17:13.749072Z","shell.execute_reply":"2023-11-29T05:17:13.857737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time_svm = time.time()\nclf = SVM(xtrain_gl, ytrain_gl, X_test, y_test, val=False, n_iters=1000)\nclf.fit()\n\nprint(clf.accuracy())\nprint(clf.precision())\nend_time_svm = time.time()\n","metadata":{"id":"lAM00vyc56lM","outputId":"cd382ed9-e68b-4872-a961-97a271d28aeb","execution":{"iopub.status.busy":"2023-11-29T05:40:23.419572Z","iopub.execute_input":"2023-11-29T05:40:23.420105Z","iopub.status.idle":"2023-11-29T05:42:32.097947Z","shell.execute_reply.started":"2023-11-29T05:40:23.420062Z","shell.execute_reply":"2023-11-29T05:42:32.096233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate and print the time taken for both methods\ntime_taken_svm = end_time_svm - start_time_svm\nprint(\"Time taken for SVM:\", time_taken_svm, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:42:32.101508Z","iopub.execute_input":"2023-11-29T05:42:32.101985Z","iopub.status.idle":"2023-11-29T05:42:32.120586Z","shell.execute_reply.started":"2023-11-29T05:42:32.101942Z","shell.execute_reply":"2023-11-29T05:42:32.117431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#start_time_fl_svm = time.time()\nf_svm = Federated_SVM(n_clients = 3, val=False, n_iters=150)\nf_svm.create_clients(clients_X,clients_y,X_test,y_test)\n\nf_svm.fit(5,f_svm.average_aggregator)\n#end_time_fl_svm = time.time()","metadata":{"id":"HoGoM0Nu26KC","outputId":"753be904-32d1-44b0-b07a-52e957864482","execution":{"iopub.status.busy":"2023-11-29T05:43:36.092373Z","iopub.execute_input":"2023-11-29T05:43:36.092735Z","iopub.status.idle":"2023-11-29T05:45:18.699463Z","shell.execute_reply.started":"2023-11-29T05:43:36.092705Z","shell.execute_reply":"2023-11-29T05:45:18.697868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Calculate and print the time taken for both methods\n# time_taken_fl_svm = end_time_fl_svm - start_time_fl_svm\n# print(\"Time taken for federated SVM:\", time_taken_fl_svm, \"seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:20:40.187919Z","iopub.execute_input":"2023-11-29T05:20:40.188276Z","iopub.status.idle":"2023-11-29T05:20:40.194596Z","shell.execute_reply.started":"2023-11-29T05:20:40.188246Z","shell.execute_reply":"2023-11-29T05:20:40.193133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_accuracy(SVM, Federated_SVM, title):\n    labels = ['Accuracy']\n\n    # For normal SVM\n    normal_svm_metrics = [SVM.accuracy()]\n\n    # For federated SVM\n    federated_svm_metrics = [Federated_SVM.accuracy_history[-1]]  # Use the accuracy after the last epoch\n\n    x = range(len(labels))\n    \n    x_normal_svm = [0]\n    x_federated_svm = [1]\n    \n    plt.figure(figsize=(6, 6))\n    bar_width = 0.5\n    plt.bar(x_normal_svm, normal_svm_metrics, width=bar_width, label='Normal SVM', align='center')\n    plt.bar(x_federated_svm, federated_svm_metrics, width=bar_width, label='Federated SVM', align='edge')\n\n    plt.ylabel('Accuracy(%)')\n    plt.title(title)\n    plt.xticks([])\n    plt.legend()\n    plt.show()\n    \n# Plot for SVM and Federated SVM\nplot_accuracy(clf, f_svm, 'SVM vs Federated SVM Accuracy')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:45:18.701127Z","iopub.execute_input":"2023-11-29T05:45:18.701477Z","iopub.status.idle":"2023-11-29T05:45:18.998869Z","shell.execute_reply.started":"2023-11-29T05:45:18.701446Z","shell.execute_reply":"2023-11-29T05:45:18.997854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_precision(SVM, Federated_SVM, title):\n    labels = ['Precision']\n\n    # For normal SVM\n    normal_svm_metrics = [SVM.precision()]\n\n    # For federated SVM\n    federated_svm_metrics = [Federated_SVM.precision_history[-1]]  # Use the accuracy after the last epoch\n\n    x = range(len(labels))\n    \n    plt.figure(figsize=(6, 6))\n    x_normal_svm_pre = [0]\n    x_federated_svm_pre = [1]\n    \n    \n    bar_width = 0.5\n    plt.bar(x_normal_svm_pre, normal_svm_metrics, width=bar_width, label='Normal SVM', align='center')\n    plt.bar(x_federated_svm_pre, federated_svm_metrics, width=bar_width, label='Federated SVM', align='edge')\n\n    plt.ylabel('Precision(%)')\n    plt.title(title)\n    plt.xticks([])\n    plt.legend(loc = 'center')\n    plt.show()\n    \n# Plot for SVM and Federated SVM\nplot_precision(clf, f_svm, 'SVM vs Federated SVM precision')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T05:45:19.000392Z","iopub.execute_input":"2023-11-29T05:45:19.001373Z","iopub.status.idle":"2023-11-29T05:45:19.324117Z","shell.execute_reply.started":"2023-11-29T05:45:19.001325Z","shell.execute_reply":"2023-11-29T05:45:19.322569Z"},"trusted":true},"execution_count":null,"outputs":[]}]}